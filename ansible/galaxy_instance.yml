---
- hosts: galaxy_instance
  become: true
  vars:
    hostname: '{{ inventory_hostname }}'
    nginx_conf_user: galaxy
    server_names:
      - '{{ hostname }}'
      - 'usegalaxy.esgwps.uno'
  vars_files:
    - group_vars/gxconfig.yml
    - group_vars/custom-sites.yml
    - secret_group_vars/db_main.yml
    - secret_group_vars/all.yml
    - templates/galaxy/config/job_conf.yml
  collections:
    - devsec.hardening
  handlers:
    - name: Restart Galaxy
      shell: |
        echo 'Manual web handler restart required' && cd /opt/galaxy/ && source /opt/galaxy/.bashrc && /usr/bin/galaxy-sync-to-nfs && systemctl restart galaxy-handler@* && systemctl restart galaxy-workflow-scheduler@*
  pre_tasks:
    - name: Install Dependencies
      package:
        name:
          [
            'git',
            'python3-psycopg2',
            'python3-virtualenv',
            'bc',
            'python38',
            'python38-devel',
            'python38-psycopg2',
            'firewalld',
          ]
      become: true
    - name: Set default version of Python
      alternatives:
        name: python
        path: /usr/bin/python3.8
    - name: Disable SELinux
      selinux:
        state: disabled
    - name: Enable and start firewalld
      service:
        name: firewalld
        state: started
        enabled: true
    - name: Allow http and https services
      firewalld:
        service: '{{ item }}'
        permanent: true
        state: enabled
        immediate: true
      loop:
        - http
        - https
    - name: Allow HTCondor ports
      firewalld:
        port: '{{ item }}'
        permanent: true
        state: enabled
        immediate: true
      loop:
        - 9618/tcp
    - name: Set sysctl fs inotify max user watches # need for celery and galaxy
      sysctl:
        name: fs.inotify.max_user_watches
        value: 1000000
        state: present
        reload: true
        sysctl_set: true
    # For restricted datasets (deferred data and other test stuff)
    - name: Create a restricted data directory
      file:
        path: '{{ galaxy_restricted_data_dir }}'
        state: directory
        owner: root
        group: root
        mode: 0755
  post_tasks:
    - name: Append some users to the systemd-journal group
      user:
        name: '{{ item }}'
        groups: systemd-journal
        append: true
      loop:
        - '{{ galaxy_user.name }}'
    - name: Set authorized SSH key (galaxy user)
      ansible.posix.authorized_key:
        user: '{{ galaxy_user.name }}'
        state: present
        key: '{{ item }}'
      loop:
        - 'ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOBINXdjILF6x3WuppXyq6J2a2oSLR6waZ6txgjYJogHdIKPbI0TdReCv4EVxxYRY/NqGpHbjkqfRTsf2VgoU3U= mk@galaxy-mira'
        - 'ecdsa-sha2-nistp521 AAAAE2VjZHNhLXNoYTItbmlzdHA1MjEAAAAIbmlzdHA1MjEAAACFBACB5Q5blymkTIRSzVzXITOGvBuI7W0L9Ykwfz8LJGPraaGVPiezzFGvjhqwX+EyCqQPt7JprR5mimJRw/JN3nBXWAHjekvmB5FuILkk6m5fOiQJ5QhRMyQ5GfxODAvGbHpTuWHbYJLWD5fhcboKPxlXOWy4xY9kDZVuQvEKisNKYBsFLA== sanjay'
        - 'ecdsa-sha2-nistp521 AAAAE2VjZHNhLXNoYTItbmlzdHA1MjEAAAAIbmlzdHA1MjEAAACFBABRaLHL8mgW86rbtdUh6TY4rs7/la8hAGeSQ3jBF7LMwYZnbS32YDMYvDq3KgNu5WqSMFvkxNm3vfTAbd8CXBfakwDBFBaD9kO0b2t4/p4VoFUsd3B2OvmTR7Bsg7OxTGJJ7aUP/SzTg+Z4NzsmHwQ9h31gfI7n/buZD4S1edQke19Y6w== dominguj@informatik.uni-freiburg.de'
    - name: Clone bio.tools database repository
      git:
        repo: https://github.com/research-software-ecosystem/content.git
        dest: '{{ biotools_content_dir }}'
      become: true
      become_user: galaxy
    - name: Create a celery directory in galaxy root directory # otherwise celery beat service will fail
      file:
        path: '{{ celery_root_dir }}'
        state: directory
        owner: galaxy
        group: galaxy
        mode: 0755

    ## if this server is going to act as the worker then,
    - name: Add groups to Galaxy user
      ansible.builtin.user:
        name: galaxy
        groups: condor,docker
        append: true
    - name: 'Deploy docker-stop-1M script'
      ansible.builtin.copy:
        content: |
          #!/bin/bash
          docker ps -f status=running --format '{{ '{{' }}.ID}}: {{ '{{' }}.RunningFor}}' | awk '/months/ && $2 > 1 { print $0 }'|cut -f1 -d':' | xargs docker stop
        dest: /usr/bin/docker-stop-1M
        owner: root
        group: root
        mode: '0755'
    - name: Ensure Docker directory extists
      ansible.builtin.file:
        state: directory
        path: /etc/docker
        mode: '0755'
    - name: Copy Docker daemon.json
      ansible.builtin.copy:
        src: compute_workers/daemon.json
        dest: /etc/docker/daemon.json
        mode: '0644'
    - name: Restart Docker service, in all cases
      ansible.builtin.service:
        name: docker
        state: restarted
  roles:
    - geerlingguy.repo-epel
    - usegalaxy_eu.handy.os_setup
    - usegalaxy-eu.autoupdates
    - influxdata.chrony
    - autofs

    ## Install miniconda, create a _galaxy_ environment and install Packages
    ## Galaxy will use the virtualenv from this conda environment (see
    ## galaxy_virtualenv_command) in the group_vars
    ## This role should come after the autofs role as the conda_prefix_dir is
    ## defined to use the tools NFS share, so it needs to be mounted first
    ## and this NFS share must also be mounted on the compute nodes
    - role: galaxyproject.miniconda
      vars:
        miniconda_prefix: '{{ conda_prefix_dir }}'
        galaxy_conda_create_env: true
        galaxy_conda_env_packages:
          - python=3.11
          - pip

    - galaxyproject.cvmfs

    # Setup Galaxy user
    - role: galaxyproject.galaxy
      vars:
        galaxy_create_user: true
        galaxy_manage_clone: false
        galaxy_manage_download: false
        galaxy_manage_cleanup: false
        galaxy_manage_existing: true
        galaxy_manage_paths: true
        galaxy_manage_static_setup: false
        galaxy_manage_mutable_setup: false
        galaxy_manage_database: false
        galaxy_manage_themes: false
        galaxy_manage_subdomain_static: false
        galaxy_fetch_dependencies: false
        galaxy_build_client: false
        galaxy_manage_gravity: false
        galaxy_manage_systemd: false

    - usegalaxy-eu.bashrc
    - usegalaxy_eu.htcondor
    - role: hxr.galaxy-nonreproducible-tools
      become: true
      become_user: galaxy
    - role: usegalaxy-eu.webhooks # Clone webhook repository
      become: true
      become_user: galaxy
    - role: usegalaxy-eu.tours # Clone tour repository
      become: true
      become_user: galaxy

    # GALAXY
    - hxr.postgres-connection
    - galaxyproject.gxadmin
    - usegalaxy_eu.ansible_nginx_upload_module
    - usegalaxy-eu.nginx

    # The REAL galaxy role
    - role: galaxyproject.galaxy
      vars:
        galaxy_create_user: true
        galaxy_manage_clone: true
        galaxy_manage_cleanup: false
        galaxy_manage_download: false
        galaxy_manage_existing: false
        galaxy_manage_static_setup: true
        galaxy_manage_mutable_setup: true
        galaxy_manage_database: true
        galaxy_manage_themes: false
        galaxy_manage_subdomain_static: false
        galaxy_manage_host_filters: false
        galaxy_manage_systemd: false
        galaxy_fetch_dependencies: true
        galaxy_build_client: true
        galaxy_manage_gravity: false

    ## Extras!
    - usegalaxy-eu.rsync-to-nfs
    - usegalaxy_eu.flower
    - usegalaxy-eu.fix-galaxy-server-dir
    - hxr.install-to-venv
    - usegalaxy_eu.galaxy_systemd
    - usegalaxy_eu.galaxy_subdomains
    - usegalaxy-eu.logrotate
    - usegalaxy-eu.error-pages
    - usegalaxy-eu.log-cleaner
    - usegalaxy_eu.tpv_auto_lint
    - devsec.hardening.ssh_hardening
    - tpv_metascheduler_api

    ## If this server is going to act as the worker then,
    - usegalaxy_eu.apptainer
    - role: geerlingguy.java
      vars:
        java_packages:
          - java-11-openjdk
          - java-11-openjdk-devel
    - role: geerlingguy.docker
      vars:
        docker_users:
          - rocky
          - condor
        docker_daemon_options: {}
    - role: usegalaxy-eu.fslimit
      vars:
        ulimit_fsize_unit: 'condor.service'
        ulimit_fsize_soft: 10740000000 # 10GB (adjust according to your storage capacity)
        ulimit_fsize_hard: 26840000000 # 25GB (adjust according to your storage capacity)

